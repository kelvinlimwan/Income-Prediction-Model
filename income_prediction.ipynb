{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Income Prediction based on U.S. Census data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Name: Kelvin Lim Wan\n",
    "\n",
    "Student ID: 929715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Info\n",
    "\n",
    "<b>Due date</b>: Friday, 16 Apr 2021 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day (both week and weekend days counted)\n",
    "\n",
    "<b>Marks</b>: 15% of mark for class \n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/105477/pages/python-and-jupyter-notebooks?module_item_id=2613813) on Canvas (under Modules>Resources) for information on the basic setup required for this class, including an iPython notebook viewer and some handy python packages including Numpy, Scipy, Matplotlib, Scikit-Learn, and Gensim. You can use any Python built-in packages, but do not use any other 3rd party packages (the packages listed above are all fine to use); if your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should edit the sections below where requested, but leave the rest of the code as is. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. While the main focus is on correctness of your methods, you will lose marks if your code is not understandable.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board; we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: For most people, collaboration will form a natural part of the undertaking of this homework, and we encourge you to discuss it in general terms with other students. However, this ultimately is still an individual task, and so reuse of code or other instances of clear influence will be considered cheating. We will be checking submissions for originality and will invoke the Universityâ€™s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this assignment, you will be working with the famous *Adult* a data set containing demographic and income data from the united states in 1994. The dataset provided for this assignment is derived from <a href=\"http://archive.ics.uci.edu/ml/datasets/Adult\">this</a> resource. The data set consists of about 48,000 individuals each characterized through a set of 14 attributes. Your task is to predict whether the individual earns up to \\\\$ 50,000 a year (<=50K) or more than \\\\$50,000 per year (>50K).\n",
    "\n",
    "The attributes are\n",
    "\n",
    "|ID|Feature Name| Feature Type | Feature Values|\n",
    "| :-| :-| :-| :-|\n",
    "|0|age| continuous| |\n",
    "|1|workclass| categorical | Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, ?|\n",
    "|2|fnlwgt| continuous| |\n",
    "|3|education|  categorical |Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool|\n",
    "|4|education-num| continuous| |\n",
    "|5|marital-status|  categorical |Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse|\n",
    "|6|occupation|  categorical |Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces, ?|\n",
    "|7|relationship|  categorical |Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried|\n",
    "|8|race|  categorical |White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black|\n",
    "|9|sex|  categorical |Female, Male|\n",
    "|10|capital-gain| continuous| |\n",
    "|11|capital-loss| continuous| |\n",
    "|12|hours-per-week| continuous| |\n",
    "|13|native-country|  categorical |United-States, Cambodia, England, Puerto-Rico, Canada, Germany, India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, ?|\n",
    "\n",
    "You can find out more about the individual attributes / values, and the origin of the data set in the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\"> data set description</a>  \n",
    "\n",
    "\n",
    "You will build a number of classifiers to predict the income class based on the attributes above.\n",
    "\n",
    "\n",
    "#### The following instructions hold for every question in the assignment\n",
    "- leave the order of instances intact, i.e., do not shuffle the data\n",
    "- do not change the names or types of variables provided by us in the code cells below\n",
    "- '?' denotes an unknown value, and you should treat it as just another value for its feature in all tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Loading and pre-processing the data (1 mark)\n",
    "\n",
    "You were provide two data files:\n",
    "\n",
    "**adult.train** contains about 32,000 training instances, one instance per line in comma-separated value (csv) format. Each line contains 14 fields. The first 13 fields correspond to the features listed above, the final field denotes the class label\n",
    "\n",
    "**adult.test** is formatted exactly like adult.train, and contains about 16,000 further instances for evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a Read the data [0.5 marks]\n",
    "\n",
    "First, you will read in the data and create traing features, training labels, test features and test labels. Do not apply any data transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32525, 32525, 16262, 16262, 14, 14)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "\n",
    "data = open(\"adult.train\",'r').readlines()\n",
    "test_data = open(\"adult.test\",'r').readlines()\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "for data_line in data:\n",
    "    \n",
    "    # create a clean string list of features; removing newlines\n",
    "    proper_data_line = data_line.rstrip(\"\\n\").split(\",\")\n",
    "    \n",
    "    # remove all whitespaces around the features\n",
    "    clean_data_line = [string.strip() for string in proper_data_line]\n",
    "    \n",
    "    features_list = clean_data_line[:-1]\n",
    "    label = clean_data_line[-1]\n",
    "    \n",
    "    x_train.append(features_list)\n",
    "    y_train.append(label)\n",
    "\n",
    "for test_line in test_data:\n",
    "    \n",
    "    # create a clean string list of features; removing newlines\n",
    "    proper_test_line = test_line.rstrip(\"\\n\").split(\",\")\n",
    "    \n",
    "    # remove all whitespaces around the features\n",
    "    clean_test_line = [string.strip() for string in proper_test_line]\n",
    "    \n",
    "    features_list = clean_test_line[:-1]\n",
    "    label = clean_test_line[-1]\n",
    "    \n",
    "    x_test.append(features_list)\n",
    "    y_test.append(label)\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "len(x_train), len(y_train), len(x_test), len(y_test), len(x_train[0]), len(x_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train)==len(y_train)==32525\n",
    "assert len(x_train[0])==len(x_test[0])==14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b: Attribute Types [0.5 marks]\n",
    "\n",
    "You will create three feature representations, based on the different attribute types (categorical, numeric) in the original *Adult* data.\n",
    "\n",
    "**Your tasks**\n",
    "\n",
    "Denote with $I$ the number of training instances; $N$ the number of numeric features in the data set; and $v_f$ the number of possible values for feature $f$.\n",
    "\n",
    "1. Create a train data set with only numeric features `x_train_num` (size $(I\\times N)$); and equivaluently a test data set `x_test_num`\n",
    "2. Create a train data set with only categorical features *in a 1-hot representation* `x_train_1hot` (size $(I\\times\\sum_f v_f)$); and equivalently a test data set `x_test_1hot`\n",
    "3. Create a train data set with both numeric and 1-hot categorical features `x_train_full` (size $I \\times N+\\sum_f v_f$) where the first $N$ columns represent the numerical features and the remaining columns the categorical features; and equivalently a test data set `x_test_full`\n",
    "\n",
    "**Note:** You may use classes and functions from ```scikit-learn```.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['State-gov', 'Bachelors', 'Never-married', 'Adm-clerical', 'Not-in-family', 'White', 'Male', 'United-States'], ['Self-emp-not-inc', 'Bachelors', 'Married-civ-spouse', 'Exec-managerial', 'Husband', 'White', 'Male', 'United-States'], ['Private', 'HS-grad', 'Divorced', 'Handlers-cleaners', 'Not-in-family', 'White', 'Male', 'United-States'], ['Private', '11th', 'Married-civ-spouse', 'Handlers-cleaners', 'Husband', 'Black', 'Male', 'United-States'], ['Private', 'Bachelors', 'Married-civ-spouse', 'Prof-specialty', 'Wife', 'Black', 'Female', 'Cuba'], ['Private', 'Masters', 'Married-civ-spouse', 'Exec-managerial', 'Wife', 'White', 'Female', 'United-States'], ['Private', '9th', 'Married-spouse-absent', 'Other-service', 'Not-in-family', 'Black', 'Female', 'Jamaica'], ['Self-emp-not-inc', 'HS-grad', 'Married-civ-spouse', 'Exec-managerial', 'Husband', 'White', 'Male', 'United-States'], ['Private', 'Masters', 'Never-married', 'Prof-specialty', 'Not-in-family', 'White', 'Female', 'United-States'], ['Private', 'Bachelors', 'Married-civ-spouse', 'Exec-managerial', 'Husband', 'White', 'Male', 'United-States']]\n",
      "[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from pandas import DataFrame\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "x_train_num = []\n",
    "x_test_num = []\n",
    "\n",
    "x_train_1hot = []\n",
    "x_test_1hot = []\n",
    "\n",
    "x_train_full = []\n",
    "x_test_full = []\n",
    "\n",
    "################################\n",
    "### Your code begins here ######\n",
    "################################\n",
    "\n",
    "NUMERIC_FEATURES_INDICES = [0, 2, 4, 10, 11, 12]\n",
    "\n",
    "## Part 1\n",
    "# for training data\n",
    "for features in x_train:\n",
    "    numeric_features = []\n",
    "    for index in NUMERIC_FEATURES_INDICES:\n",
    "        numeric_features.append(int(features[index]))\n",
    "    x_train_num.append(numeric_features)\n",
    "\n",
    "# for test data\n",
    "for features in x_test:\n",
    "    numeric_features = []\n",
    "    for index in NUMERIC_FEATURES_INDICES:\n",
    "        numeric_features.append(int(features[index]))\n",
    "    x_test_num.append(numeric_features)\n",
    "\n",
    "    \n",
    "## Part 2\n",
    "# for training data\n",
    "cat_x_train = deepcopy(x_train)\n",
    "\n",
    "# remove all numeric features\n",
    "for features in cat_x_train:\n",
    "    for index in reversed(NUMERIC_FEATURES_INDICES):\n",
    "        del features[index]\n",
    "        \n",
    "# create a pandas dataframe\n",
    "cat_x_train_df = DataFrame(cat_x_train)\n",
    "\n",
    "# convert the categorical features into one-hot representation\n",
    "train_encoder = OneHotEncoder()\n",
    "cat_x_train_one_hot = train_encoder.fit_transform(cat_x_train_df).toarray()\n",
    "for one_hot_features in cat_x_train_one_hot:\n",
    "    x_train_1hot.append([int(feat) for feat in one_hot_features])\n",
    "\n",
    "# for test data\n",
    "cat_x_test = deepcopy(x_test)\n",
    "\n",
    "# remove all numeric features\n",
    "for features in cat_x_test:\n",
    "    for index in reversed(NUMERIC_FEATURES_INDICES):\n",
    "        del features[index]\n",
    "        \n",
    "# create a pandas dataframe\n",
    "cat_x_test_df = DataFrame(cat_x_test)\n",
    "\n",
    "# convert the categorical features into one-hot a representation\n",
    "test_encoder = OneHotEncoder()\n",
    "cat_x_test_one_hot = test_encoder.fit_transform(cat_x_test_df).toarray()\n",
    "for one_hot_features in cat_x_test_one_hot:\n",
    "    x_test_1hot.append([int(feat) for feat in one_hot_features])\n",
    "\n",
    "    \n",
    "## Part 3\n",
    "# for training data\n",
    "for i in range(len(x_train_num)):\n",
    "    x_train_full.append(x_train_num[i] + x_train_1hot[i])\n",
    "    \n",
    "# for test data\n",
    "for i in range(len(x_test_num)):\n",
    "    x_test_full.append(x_test_num[i] + x_test_1hot[i])\n",
    "\n",
    "################################\n",
    "### Your code ends here ########\n",
    "################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> For your testing</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(x_train_1hot[0])==98\n",
    "assert len(x_train_full[0])==104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: A 0-R baseline [0.5 marks]\n",
    "\n",
    "Implement a zero-r baseline, as introduced in the Evaluation lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority class is: <=50K\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "majority_class = \"\"\n",
    "zero_r_predictions = []\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "# create a dictionary of labels with their frequency in the training labels\n",
    "label_counts = Counter()\n",
    "for label in y_train:\n",
    "    label_counts[label] += 1\n",
    "    \n",
    "# get the most common label\n",
    "majority_class = label_counts.most_common()[0][0]\n",
    "\n",
    "for features in x_test:\n",
    "    zero_r_predictions.append(majority_class)\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(f\"The majority class is: {majority_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Feature selection [2 marks]\n",
    "\n",
    "In this question you will implement pointwise mutual information (PMI) for feature selection (Question 3a)\n",
    "\n",
    "In question 3a. you will use the implemented function to create a 1-R classifier based on the single 1-hot attribute  (i.e., categorical feature value) with highest PMI for class \">50K\". \n",
    "```\n",
    "argmax_a pmi(a=1,c=\">50K\")\n",
    "```\n",
    "In question 3b. you will apply your 1-R classifier to the test instances, and store your predicted labels in `one_r_predictions`.\n",
    "\n",
    "<b> You should implement PMI from scratch yourself. You may use native Python libraries like math or numpy to help you, but you may not use existing implementations of PMI.</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Implement PMI [1 mark]\n",
    "\n",
    "Implement a function to compute PMI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PMI: A function to compute the pointwise mutual information between \n",
    "all features in the data set and a target class of interest\n",
    "Input: - features for N input instances\n",
    "       - labels for N input instances\n",
    "       - target feature value (default=1)\n",
    "       - target class (fdefault='>50K')\n",
    "Output: a dictionary of the form {feature: ppmi} where each \n",
    "        feature is denoted by its ID (position in the 1-hot encoded representatoin)\n",
    "\n",
    "'''\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "from math import inf, log\n",
    "\n",
    "def pmi(features, labels, tgt_value=1, tgt_class=\">50K\"):\n",
    "    \n",
    "    pmis = {}\n",
    "    \n",
    "    # probability of C\n",
    "    prob_label = labels.count(tgt_class) / len(labels)\n",
    "        \n",
    "    for i in range(len(features[0])):\n",
    "        one_count = 0\n",
    "        joint_count = 0\n",
    "        index = 0\n",
    "        for feat in features:\n",
    "            if feat[i] == tgt_value:\n",
    "                one_count += 1\n",
    "                if labels[index] == tgt_class:\n",
    "                    joint_count += 1\n",
    "            index += 1\n",
    "            \n",
    "        # probability of A\n",
    "        prob_feat = one_count / len(features)\n",
    "        \n",
    "        # joing probability of A and C\n",
    "        prob_joint = joint_count / len(features)\n",
    "        \n",
    "        # set PMI to infinity if denominator is 0 since log of infinity is infinity\n",
    "        if prob_feat == 0 or prob_label == 0:\n",
    "            pmis[i] = inf\n",
    "            continue\n",
    "        \n",
    "        argument = prob_joint / (prob_feat * prob_label)\n",
    "        \n",
    "        # set PMI to minus infinity if argument is 0 since log of 0 is minus infinity\n",
    "        if argument == 0:\n",
    "            pmis[i] = - inf\n",
    "        else:\n",
    "            pmis[i] = log(argument, 2)\n",
    "               \n",
    "    return pmis\n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For your testing:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = [[1,1], [1,0], [0,1], [0,0]]\n",
    "test_labels = [1,1,0,0]\n",
    "test_pmi = pmi(test_features, test_labels, tgt_class=1)\n",
    "\n",
    "# where the index 0 refers to \"feature 1\" and index 1 refers to \"feature 2\"\n",
    "assert test_pmi[0]==1.0\n",
    "assert test_pmi[1]==0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b Create 1-R baseline [1 mark]\n",
    "\n",
    "- Apply your PMI function to the *1_hot feature representation* of training data, and determine the (i) 1-hot feature with *highest* PMI with class '>50K' () and (ii) 1-hot feature with *lowest* (most negative) PMI with class '>50K'. Store the name (string) of the corresponding highest/lowest PMI features in `highest_pmi_feature_name` and `lowest_pmi_feature_name`, respectively.\n",
    "\n",
    "- The feature with *highest* PMI will consitute your 1-R predictor, which you should use to predict the class labels for the test set (`one_r_predictions`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<=50K', '<=50K', '<=50K', '<=50K', '<=50K', '<=50K', '<=50K', '<=50K', '<=50K', '<=50K']\n",
      "The feature with highest PMI for the class '>50K' is: Doctorate\n",
      "The feature with lowest PMI for the class '>50K' is: Preschool\n"
     ]
    }
   ],
   "source": [
    "one_r_predictions = []\n",
    "\n",
    "highest_pmi_feature_name = \"\" # feature with highest PMI\n",
    "lowest_pmi_feature_name = \"\" # feature with lowest PMI\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "FEATURE_NAME_START_INDEX = 3\n",
    "TARGET_CLASS = \">50K\"\n",
    "OTHER_CLASS = \"<=50K\"\n",
    "TARGET_VALUE = 1\n",
    "\n",
    "# compute the pmi's of each feature as a dictionary\n",
    "feature_pmis = pmi(x_train_1hot, y_train, tgt_value=TARGET_VALUE, tgt_class=TARGET_CLASS)\n",
    "\n",
    "# sort the features in ascending order of pmi\n",
    "sorted_pmis = sorted(feature_pmis.items(), key=lambda x:x[1])\n",
    "\n",
    "highest_pmi_index = sorted_pmis[-1][0]\n",
    "lowest_pmi_index = sorted_pmis[0][0]\n",
    "\n",
    "# list of all feature names ordered the same as in the one hot encoder\n",
    "features_names = list(train_encoder.get_feature_names())\n",
    "\n",
    "highest_pmi_feature_name = features_names[highest_pmi_index][FEATURE_NAME_START_INDEX:]\n",
    "lowest_pmi_feature_name = features_names[lowest_pmi_index][FEATURE_NAME_START_INDEX:]\n",
    "\n",
    "for features in x_test_1hot:\n",
    "    if features[highest_pmi_index] == TARGET_VALUE:\n",
    "        one_r_predictions.append(TARGET_CLASS)\n",
    "    else:\n",
    "        one_r_predictions.append(OTHER_CLASS)\n",
    "        \n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(one_r_predictions[:10])\n",
    "print(f\"The feature with highest PMI for the class '>50K' is: {highest_pmi_feature_name}\")\n",
    "print(f\"The feature with lowest PMI for the class '>50K' is: {lowest_pmi_feature_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Naive Bayes [3 marks]\n",
    "\n",
    "We will construct three Naive Bayes classifiers\n",
    "\n",
    "1. One for instances with only 1-hot encoded (binary) categorical attributes.\n",
    "2. One for instances with only numerical attributes.\n",
    "3. One for instances with the full set of numerical *and* categorical attributes, ensuring that your classifier computes posterior class probabilities $p(y|x)$ as $p(x|y)p(y)$.\n",
    "\n",
    "For each classifier, you will \n",
    "1. Train it on the training set\n",
    "2. Use the models to predict labels for the test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a Implementing the Naive Bayes classifiers [1.5 marks]\n",
    "\n",
    "Implement three functions which train a NB classifier given the specified input feature types and predict labels for a given test set.\n",
    "\n",
    "You may add additional quantities to your `return` statements.\n",
    "\n",
    "<b> You may (and are, indeed, encouraged) to, use the existing NB implementations from `sklearn`. You should use the default parameterizations of these algorithms.</b> \n",
    "    \n",
    "If you choose to implement your classifiers from scratch, please use Laplace smoothing (alpha=1) for the categorical feature NB.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################\n",
    "## your code begins here\n",
    "##################################\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "\n",
    "BINARY_FEATURES_START_INDEX = 6\n",
    "CLASS_1 = \"<=50K\"\n",
    "CLASS_2 = \">50K\"\n",
    "CLASS_1_INDEX = 0\n",
    "CLASS_2_INDEX = 1\n",
    "\n",
    "def nb_binary_features(train_features, train_labels, test_features):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # fit the bernoulli NB classifier to the training data using laplace smoothing of alpha 1 (default)\n",
    "    classifier = BernoulliNB()\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    # predict test labels by passing the test binary features into the bernoulli NB classifier\n",
    "    predictions = list(classifier.predict(test_features))\n",
    "    \n",
    "    # get the posterior class probabilities for each feature\n",
    "    post_probs = [list(prob) for prob in classifier.predict_proba(test_features)]\n",
    "    \n",
    "    return predictions, post_probs\n",
    "\n",
    "\n",
    "def nb_continuous_features(train_features, train_labels, test_features):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # fit the gaussian NB classifier to the training data using laplace smoothing of alpha 1 (default)\n",
    "    classifier = GaussianNB()\n",
    "    classifier.fit(train_features, train_labels)\n",
    "    \n",
    "    # predict test labels by passing the test continuous features into the gaussian NB classifier\n",
    "    predictions = list(classifier.predict(test_features))\n",
    "    \n",
    "    # get the posterior class probabilities for each feature\n",
    "    post_probs = [list(prob) for prob in classifier.predict_proba(test_features)]\n",
    "    \n",
    "    return predictions, post_probs\n",
    "\n",
    "\n",
    "def nb_full(train_features, train_labels, test_features):    \n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # slice the features list into continuous and binary features lists\n",
    "    train_features_num = [features[:BINARY_FEATURES_START_INDEX] for features in train_features]\n",
    "    train_features_1hot = [features[BINARY_FEATURES_START_INDEX:] for features in train_features]\n",
    "    test_features_num = [features[:BINARY_FEATURES_START_INDEX] for features in test_features]\n",
    "    test_features_1hot = [features[BINARY_FEATURES_START_INDEX:] for features in test_features]\n",
    "    \n",
    "    # get the prior class probabilities\n",
    "    prob_less_or_equal_to_50k = train_labels.count(CLASS_1) / len(train_labels)\n",
    "    prob_greater_than_50k = train_labels.count(CLASS_2) / len(train_labels)\n",
    "\n",
    "    # get the posterior class probabilities for each feature\n",
    "    post_probs_num = nb_continuous_features(train_features_num, train_labels, test_features_num)[1]\n",
    "    post_probs_1hot = nb_binary_features(train_features_1hot, train_labels, test_features_1hot)[1]\n",
    "    \n",
    "    for i in range(len(post_probs_num)):\n",
    "        # compute the \"full\" posterior probabilities by multiplying the continuous and binary post probabilities and\n",
    "        # dividing it by the prior class probability, for each feature: P(Y|X_FULL) = P(Y) * P(X_FULL|Y) =\n",
    "        # P(Y) * P(X_NUM|Y) * P(X_1HOT|Y) = P(Y) * P(X_NUM|Y) * P(Y) * P(X_1HOT|Y) / P(Y)\n",
    "        # = P(Y|X_NUM) * P(Y|X_1HOT) / P(Y)\n",
    "        post_prob_less_equal = (post_probs_num[i][CLASS_1_INDEX] * post_probs_1hot[i][CLASS_1_INDEX] / \n",
    "                                prob_less_or_equal_to_50k)\n",
    "        post_prob_greater = (post_probs_num[i][CLASS_2_INDEX] * post_probs_1hot[i][CLASS_2_INDEX] / \n",
    "                             prob_greater_than_50k)\n",
    "        \n",
    "        # assign label to the one with highest posterior probability\n",
    "        if post_prob_less_equal > post_prob_greater:\n",
    "            predictions.append(CLASS_1)\n",
    "        else:\n",
    "            predictions.append(CLASS_2)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b Apply your classifiers to the data sets you created in Questions 1 and 3b [0.5 marks]\n",
    "\n",
    "...namely, the\n",
    "\n",
    "1. 1-hot categorical features `x_{train,test}_1hot`\n",
    "2. numerical features `x_{train,test}_num`\n",
    "3. combined numerical and 1-hot categorical features `x_{train,test}_full`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical NB predicted class distribution \t Counter({'<=50K': 14440, '>50K': 1822})\n",
      "Categorical NB predicted class distribution\t Counter({'<=50K': 10046, '>50K': 6216})\n",
      "Full NB predicted class distribution\t Counter({'<=50K': 12699, '>50K': 3563})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "categorical_nb_predictions = nb_binary_features(x_train_1hot, y_train, x_test_1hot)[0]\n",
    "numeric_nb_predictions = nb_continuous_features(x_train_num, y_train, x_test_num)[0]\n",
    "full_nb_predictions = nb_full(x_train_full, y_train, x_test_full)\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(f\"Numerical NB predicted class distribution \\t {Counter(numeric_nb_predictions)}\")\n",
    "print(f\"Categorical NB predicted class distribution\\t {Counter(categorical_nb_predictions)}\")\n",
    "print(f\"Full NB predicted class distribution\\t {Counter(full_nb_predictions)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4c Explain your Naive Bayes classifier implementation on the full attribute set (numerical and binary features). [1 mark]\n",
    "\n",
    "Please limit your answer to 2-3 sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous functions, I retrieved the posterior probabilities for both the numerical dataset and the one-hot-encoded dataset. For each instance, I multiplied the two \"partial\" posterior probabilities and divided it by the prior class probability, for each class. Then I assigned each instance prediction to the label with the higher \"full\" posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Logistic Regression [2.5 marks]\n",
    "\n",
    "Apply a Logistic Regression classifier to the full training data set (`x_{train,test}_full`)\n",
    "\n",
    "<b> Use the existing implementation in sklearn with default parameters. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a The Logistic Regression classifier [0.5 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature with the highest PMI\t Doctorate\n",
      "Feature with the second highest PMI\t Prof-school\n",
      "Feature with the lowest PMI\t Preschool\n",
      "Feature with the second lowest PMI\t Priv-house-serv\n",
      "Feature with the highest coefficient\t Capital-loss\n",
      "Feature with the second highest coefficient\t Married-civ-spouse\n",
      "LR predicted class distribution\t Counter({'<=50K': 14837, '>50K': 1425})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_predictions = []\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "## Part a\n",
    "# fit the LR classifier to the training data\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(x_train_full, y_train)\n",
    "\n",
    "# predict test labels by passing the test features into the LR classifier\n",
    "lr_predictions = classifier.predict(x_test_full)\n",
    "\n",
    "\n",
    "## Part b\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# ___ to be consistent with the slicing using in Q3b\n",
    "CONTINUOUS_FEATURES_NAMES = [\"___Age\", \"___Fnlwgt\", \"___Education-num\", \"___Capital-gain\", \"___Capital-loss\", \"___Hours-per-week\"]\n",
    "\n",
    "# get LR coefficients\n",
    "parameters = classifier.coef_\n",
    "all_coef = list(parameters[0])\n",
    "continuous_coef = all_coef[:BINARY_FEATURES_START_INDEX]\n",
    "binary_coef = all_coef[BINARY_FEATURES_START_INDEX:]\n",
    "\n",
    "# get feature pmi's\n",
    "pmis = [tup[1] for tup in feature_pmis.items()]\n",
    "\n",
    "# plot coefficients vs pmi's for binary features only since pmi's only work on binary features\n",
    "pyplot.plot(binary_coef, pmis)\n",
    "pyplot.xlabel(\"Coefficients of binary features\")\n",
    "pyplot.ylabel(\"PMI's of binary features\")\n",
    "pyplot.show()\n",
    "\n",
    "# get the two features with the highest pmi's\n",
    "highest_pmi_index = sorted_pmis[-1][0]\n",
    "second_highest_pmi_index = sorted_pmis[-2][0]\n",
    "highest_pmi_feature_name = features_names[highest_pmi_index][FEATURE_NAME_START_INDEX:]\n",
    "second_highest_pmi_feature_name = features_names[second_highest_pmi_index][FEATURE_NAME_START_INDEX:]\n",
    "\n",
    "# get the two features with the lowest pmi's\n",
    "lowest_pmi_index = sorted_pmis[0][0]\n",
    "second_lowest_pmi_index = sorted_pmis[1][0]\n",
    "lowest_pmi_feature_name = features_names[lowest_pmi_index][FEATURE_NAME_START_INDEX:]\n",
    "second_lowest_pmi_feature_name = features_names[second_lowest_pmi_index][FEATURE_NAME_START_INDEX:]\n",
    "\n",
    "# list of all feature names\n",
    "all_features_names = CONTINUOUS_FEATURES_NAMES + features_names\n",
    "\n",
    "# get the two features with the highest LR coefficients\n",
    "coefficients = [(i, all_coef[i]) for i in range(len(all_coef))]\n",
    "sorted_coefficients = sorted(coefficients, key=lambda x:x[1])\n",
    "highest_coef_index = sorted_coefficients[-1][0]\n",
    "second_highest_coef_index = sorted_coefficients[-2][0]\n",
    "highest_coef_feature_name = all_features_names[highest_coef_index][FEATURE_NAME_START_INDEX:]\n",
    "second_highest_coef_feature_name = all_features_names[second_highest_coef_index][FEATURE_NAME_START_INDEX:]\n",
    "\n",
    "print(f\"Feature with the highest PMI\\t {highest_pmi_feature_name}\")\n",
    "print(f\"Feature with the second highest PMI\\t {second_highest_pmi_feature_name}\")\n",
    "print(f\"Feature with the lowest PMI\\t {lowest_pmi_feature_name}\")\n",
    "print(f\"Feature with the second lowest PMI\\t {second_lowest_pmi_feature_name}\")\n",
    "print(f\"Feature with the highest coefficient\\t {highest_coef_feature_name}\")\n",
    "print(f\"Feature with the second highest coefficient\\t {second_highest_coef_feature_name}\")\n",
    "\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(f\"LR predicted class distribution\\t {Counter(lr_predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b Inspect the learnt coefficients [2 marks]\n",
    "\n",
    "**Part 1**\n",
    "Inspect the coefficients (weights) learnt by the classifier, and compare them to the PMI values in Question 1. You should update your code above in order to achieve this. Please indicate through comments which parts of the code are relevant to question 5b.\n",
    "\n",
    "**Part 2**\n",
    "What does a high coefficient for a feature imply? For which two features does your model learn the largest coefficients? Compare your answer to the highest PMI features you detected in Q3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high coefficient implies that the feature is significant and powerful in predicting the instance label. For example, if age has a high coefficient and the target class (class that is attributed the binary number 1) is \">50K\", all else being equal, an old person (high age) is more likely to have it's income classified as \">50K\".\n",
    "\n",
    "The two features with the largest coefficients are Capital-loss and Married-civ-spouse. \n",
    "The two features with the highest PMI's are Doctorate and Prof-school.\n",
    "The two features with the lowest PMI's are Preschool and Priv-house-serv\n",
    "\n",
    "For the logistic regression, the class that is attributed the binary number 1 (target class) is \"<=50K\" since LogisticRegression from scikit-learn assigns the first label in the input as the target class. For PMI, the default target class used in Q3 is \">50K\". Hence, the features are not comparable. \n",
    "\n",
    "I have computed the two features with the lowest PMI's (highest PMI's for target class \"<=50K\"); let's compare the features with the highest LR coefficient and lowest PMI for simplicity: it makes sense that a high Capital-loss is correlated with \"<=50K\" (a low income) and similarly for the Preschool feature. In contrast, the Doctorate feature correlates with \">50K\" (a high income), which is also makes sense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6: Evaluation [1 mark]\n",
    "\n",
    "We will evaluate our baselines and classifiers on the instances in the test set. \n",
    "\n",
    "Compute \n",
    "- accuracy\n",
    "- macro-averaged F1 score \n",
    "\n",
    "for the two baselines, the three NB classifiers and the LR classifier.\n",
    "\n",
    "**You may use existing implementations and/or Python libraries like numpy, scipy or sklearn.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero R\t\tAccuracy: 0.76\tMacro F1: 0.43\n",
      "One  R\t\tAccuracy: 0.77\tMacro F1: 0.46\n",
      "NB Num \t\tAccuracy: 0.8\tMacro F1: 0.64\n",
      "NB Cat \t\tAccuracy: 0.76\tMacro F1: 0.72\n",
      "NB Full \tAccuracy: 0.83\tMacro F1: 0.76\n",
      "LR \t\tAccuracy: 0.8\tMacro F1: 0.63\n"
     ]
    }
   ],
   "source": [
    "zero_r_acc = 0\n",
    "zero_r_f1 = 0\n",
    "\n",
    "one_r_acc = 0\n",
    "one_r_f1 = 0\n",
    "\n",
    "num_nb_acc = 0\n",
    "num_nb_f1 = 0\n",
    "\n",
    "cat_nb_acc = 0\n",
    "cat_nb_f1 = 0\n",
    "\n",
    "full_nb_acc = 0\n",
    "full_nb_f1 = 0\n",
    "\n",
    "lr_acc = 0\n",
    "lr_f1 = 0\n",
    "\n",
    "############################\n",
    "## your code begins here\n",
    "############################\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# zero r\n",
    "zero_r_acc = accuracy_score(y_test, zero_r_predictions)\n",
    "zero_r_f1 = f1_score(y_test, zero_r_predictions, average=\"macro\")\n",
    "\n",
    "# one r\n",
    "one_r_acc = accuracy_score(y_test, one_r_predictions)\n",
    "one_r_f1 = f1_score(y_test, one_r_predictions, average=\"macro\")\n",
    "\n",
    "# numerical NB\n",
    "num_nb_acc = accuracy_score(y_test, numeric_nb_predictions)\n",
    "num_nb_f1 = f1_score(y_test, numeric_nb_predictions, average=\"macro\")\n",
    "\n",
    "# binary NB\n",
    "cat_nb_acc = accuracy_score(y_test, categorical_nb_predictions)\n",
    "cat_nb_f1 = f1_score(y_test, categorical_nb_predictions, average=\"macro\")\n",
    "\n",
    "# full NB\n",
    "full_nb_acc = accuracy_score(y_test, full_nb_predictions)\n",
    "full_nb_f1 = f1_score(y_test, full_nb_predictions, average=\"macro\")\n",
    "\n",
    "# LR\n",
    "lr_acc = accuracy_score(y_test, lr_predictions)\n",
    "lr_f1 = f1_score(y_test, lr_predictions, average=\"macro\")\n",
    "\n",
    "############################\n",
    "## your code ends here\n",
    "############################\n",
    "\n",
    "print(f\"Zero R\\t\\tAccuracy: {round(zero_r_acc, 2)}\\tMacro F1: {round(zero_r_f1, 2)}\")\n",
    "print(f\"One  R\\t\\tAccuracy: {round(one_r_acc, 2)}\\tMacro F1: {round(one_r_f1, 2)}\")\n",
    "print(f\"NB Num \\t\\tAccuracy: {round(num_nb_acc, 2)}\\tMacro F1: {round(num_nb_f1, 2)}\")\n",
    "print(f\"NB Cat \\t\\tAccuracy: {round(cat_nb_acc, 2)}\\tMacro F1: {round(cat_nb_f1, 2)}\")\n",
    "print(f\"NB Full \\tAccuracy: {round(full_nb_acc, 2)}\\tMacro F1: {round(full_nb_f1, 2)}\")\n",
    "print(f\"LR \\t\\tAccuracy: {round(lr_acc, 2)}\\tMacro F1: {round(lr_f1, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: Discussion [5 marks]\n",
    "\n",
    "Critically analyze the performance of the models by answering the following questions. \n",
    "\n",
    "**(a)** The three Naive Bayes (NB) classifiers lead to different performance. Which of the three NB classifiers performs best, and why do you think it is the case? **[1 mark]**\n",
    "\n",
    "**(b)** There is a systematic difference between Accuracy and F1 score, across all models. Describe and explain the difference in the context of the data set. **[1 mark]**\n",
    "\n",
    "**(c)** Assume that your best classifier will be deployed in a bank to decide whether a customer will be granted a loan, or not. A loan will be granted if the classifier predicts an income of \">50K\", othewise it won't. The bank manager wants to avoid falsely granting a loan to an applicant with insufficient income by all means. Can you reassure the bank manager with the evaluation results in Q7 that your classifier is adequate? If not, describe an alternative evaluation metric and explain why it is more appropriate.\n",
    "\n",
    "**(d)** Do you observe a clear difference in performance between NB and Logistic Regression (LR) in Q7? Referring to the number of parameters and assumptions underlying each model, provide one reason why NB might outperform LR, and one reason why LR might outperform NB. In your answer, refer back to the *Adult* data set used in this assignment. **[1 mark]**\n",
    "\n",
    "**(e)** Assume that you have access to a (hypothetical) infinitely large U.S. census database (i.e., an enormous version of the *Adult* data set used in this assignment). Is it true that Logistic Regression, but not Naive Bayes, will achieve perfect test set performance? Why (not)? **[1 mark]**\n",
    "\n",
    "<b>We expect a maximum of 2-3 sentences per questinon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) The full NB classifier performs best because it captures all the features and uses them to predict the labels. In contrast, the continuous and binary NB classifiers only use a fraction of the features to predict the labels. The full NB classifier is the most informative and results in the highest accuracy and F1 score since it has the most data points (features).\n",
    "\n",
    "(b) Accuracy puts more emphasis on the true positives and true negatives in the dataset whereas F1 score captures the false postivies and false negatives more. Accuracy is more informative when the class distribution is similar while F1 score is a better metric when there are imbalanced classes in the dataset.\n",
    "\n",
    "(c) The higher the F1 score, the less likely will the bank manager falsely grant a loan to an applicant. A more suitable metric is the False Positive Rate. It computes the ratio of false positives to the number of incorrectly labelled instances. Hence it would highlight the rate of falsely granting a loan rather than also highlighting the rate of falsely not granting a loan (false negative), which is what the F1 score captures.\n",
    "\n",
    "(d) For this dataset, the full NB classifier outperforms the LR model by a small margin. They both use the same number of parameters (104- the number of features) but their model assumptions differ. NB will likely outperform LR if there is a small training data since NB is based on the joint density function whereas LR estimates may overfit the data. For these same reasons, LR will likely outperform NB for a large training data. Since the binary features are broken down into one-hot-representation in the Adult dataset, the features are most likely conditionally independent, and hence NB outperforms LR even though the dataset is large.\n",
    "\n",
    "(e) Assume that the training and test data sets come from the same empirical data and there is no outlier in the data. With an infinitely large dataset, the loss function of a LR model will be completely (optimally) minimised. Hence, the LR coefficients will be 100% accurate and will achieve perfect test set performance. This is not true if the training and test data sets come from distinct empirical data or there are outliers in the data. A NB classifier still won't achieve perfect results because of it's strong assumption on conditional independence which doesn't disappear over large data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts ...\n",
    "\n",
    "Do you think it is ethical and safe to build and deploy income-level classifiers based on the set of attributes provided with the data set? Could such a classifier do harm if not developed, tested and deployed carefully?\n",
    "\n",
    "When predicting the income of a person, should demographic attributes like 'gender' or 'race' even be considered? Due to historical events there are systematic income differences between men/women and people of different ethnicities. But no doubt these are *correlations* and not factors to be used to *predict* income in the future. \n",
    "\n",
    "But ML models are only as good as their input data: a naively developed ML algorithm will reflect, and often amplify, biases in the data.\n",
    "\n",
    "In the last 1-2 weeks of this subject we will discuss ethics in machine learning and look at questions such as\n",
    "- historical artifacts which lead to biased data sets, and hence potentially biased ML algorithms\n",
    "- how to determine whether an algorithm is indeed biased\n",
    "- how to mitigate bias in the data and in algorithms\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
